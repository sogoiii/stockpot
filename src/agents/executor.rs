//! Agent executor - runs agents using serdesAI's Agent API.
//!
//! This module provides the execution layer for SpotAgents, using
//! serdesAI's agent loop with proper tool calling and streaming support.

use crate::agents::{SpotAgent, AgentManager};
use crate::auth;
use crate::config::Settings;
use crate::db::Database;
use crate::mcp::McpManager;
use crate::messaging::{EventBridge, MessageSender};
use crate::models::{resolve_api_key, ModelRegistry, ModelType};
use crate::tools::registry::SpotToolRegistry;
use crate::tools::agent_tools::InvokeAgentTool;
use tracing::{debug, error, info, warn};

use serdes_ai_agent::{agent, RunOptions};
use serdes_ai_core::{ModelRequest, ModelResponse, ModelSettings};
use serdes_ai_models::{
    infer_model, Model, ModelError, ModelProfile, ModelRequestParameters, StreamedResponse,
    openai::OpenAIChatModel,
};
use serdes_ai_tools::{Tool, ToolDefinition, ToolReturn, ToolError, RunContext};

use async_trait::async_trait;
use futures::StreamExt;
use serde_json::Value as JsonValue;
use std::sync::Arc;
use tokio::sync::mpsc;

// Re-export stream event for consumers
pub use serdes_ai_agent::AgentStreamEvent as StreamEvent;

/// Wrapper to make Arc<dyn Model> implement Model.
/// 
/// This allows us to use dynamically dispatched models with serdesAI's
/// agent builder, which requires a concrete Model type.
struct ArcModel(Arc<dyn Model>);

#[async_trait]
impl Model for ArcModel {
    fn name(&self) -> &str {
        self.0.name()
    }

    fn system(&self) -> &str {
        self.0.system()
    }

    fn identifier(&self) -> String {
        self.0.identifier()
    }

    async fn request(
        &self,
        messages: &[ModelRequest],
        settings: &ModelSettings,
        params: &ModelRequestParameters,
    ) -> Result<ModelResponse, ModelError> {
        self.0.request(messages, settings, params).await
    }

    async fn request_stream(
        &self,
        messages: &[ModelRequest],
        settings: &ModelSettings,
        params: &ModelRequestParameters,
    ) -> Result<StreamedResponse, ModelError> {
        self.0.request_stream(messages, settings, params).await
    }

    fn profile(&self) -> &ModelProfile {
        self.0.profile()
    }

    async fn count_tokens(&self, messages: &[ModelRequest]) -> Result<u64, ModelError> {
        self.0.count_tokens(messages).await
    }
}

/// Wrapper that adapts an `Arc<dyn Tool>` to work as a `ToolExecutor<()>`.
/// 
/// This bridges our Tool implementations (which use `call()`) to
/// serdesAI's executor interface (which uses `execute()`).
struct ToolExecutorAdapter {
    tool: Arc<dyn Tool + Send + Sync>,
}

impl ToolExecutorAdapter {
    fn new(tool: Arc<dyn Tool + Send + Sync>) -> Self {
        Self { tool }
    }
}

#[async_trait]
impl serdes_ai_agent::ToolExecutor<()> for ToolExecutorAdapter {
    async fn execute(
        &self,
        args: JsonValue,
        ctx: &serdes_ai_agent::RunContext<()>,
    ) -> Result<ToolReturn, ToolError> {
        // Convert serdes_ai_agent::RunContext to serdes_ai_tools::RunContext
        let tool_ctx = RunContext::minimal(&ctx.model_name);
        self.tool.call(&tool_ctx, args).await
    }
}

/// Executor for invoke_agent that has access to all required dependencies.
struct InvokeAgentExecutor {
    db_path: std::path::PathBuf,
    current_model: String,
    /// Optional message bus for sub-agent event publishing.
    bus: Option<MessageSender>,
}

impl InvokeAgentExecutor {
    /// Create executor with message bus (preferred - enables visible sub-agent output).
    fn new(db: &Database, current_model: &str, bus: MessageSender) -> Self {
        Self {
            db_path: db.path().to_path_buf(),
            current_model: current_model.to_string(),
            bus: Some(bus),
        }
    }

    /// Create executor without message bus (legacy - sub-agent output not visible).
    fn new_legacy(db: &Database, current_model: &str) -> Self {
        Self {
            db_path: db.path().to_path_buf(),
            current_model: current_model.to_string(),
            bus: None,
        }
    }

    fn definition() -> ToolDefinition {
        InvokeAgentTool::default().definition()
    }
}

#[async_trait]
impl serdes_ai_agent::ToolExecutor<()> for InvokeAgentExecutor {
    async fn execute(
        &self,
        args: JsonValue,
        _ctx: &serdes_ai_agent::RunContext<()>,
    ) -> Result<ToolReturn, ToolError> {
        #[derive(serde::Deserialize)]
        struct Args {
            agent_name: String,
            prompt: String,
            #[serde(default)]
            session_id: Option<String>,
        }

        let args: Args = serde_json::from_value(args.clone()).map_err(|e| {
            ToolError::execution_failed(format!("Invalid arguments: {}", e))
        })?;

        debug!(agent = %args.agent_name, "Invoking sub-agent");

        // Clone the data we need for the blocking task
        let db_path = self.db_path.clone();
        let current_model = self.current_model.clone();
        let agent_name = args.agent_name.clone();
        let prompt = args.prompt.clone();
        let session_id = args.session_id.clone();
        let bus = self.bus.clone(); // Clone bus for the spawned task

        // Run the agent in a blocking context to handle the non-Send Database
        let result = tokio::task::spawn_blocking(move || {
            // Create a new runtime for the blocking task
            let rt = tokio::runtime::Builder::new_current_thread()
                .enable_all()
                .build()
                .map_err(|e| format!("Failed to create runtime: {}", e))?;

            rt.block_on(async {
                // Open a fresh database connection
                let db = Database::open_at(db_path)
                    .map_err(|e| format!("Failed to open database: {}", e))?;

                // Load fresh registries
                let model_registry = ModelRegistry::load_from_db(&db).unwrap_or_default();
                let agent_manager = AgentManager::new();
                let tool_registry = SpotToolRegistry::new();
                let mcp_manager = McpManager::new();

                // Find the agent
                let agent = agent_manager
                    .get(&agent_name)
                    .ok_or_else(|| format!("Agent not found: {}", agent_name))?;

                // Get the effective model for this agent (pinned or current)
                let effective_model = {
                    let settings = Settings::new(&db);
                    settings
                        .get_agent_pinned_model(&agent_name)
                        .unwrap_or_else(|| current_model.clone())
                };

                // Create executor - with bus if available for visible sub-agent output
                let executor = AgentExecutor::new(&db, &model_registry);

                let result = if let Some(bus) = bus {
                    // Use execute_with_bus - events flow to the same bus!
                    executor
                        .with_bus(bus)
                        .execute_with_bus(
                            agent,
                            &effective_model,
                            &prompt,
                            None,
                            &tool_registry,
                            &mcp_manager,
                        )
                        .await
                        .map_err(|e| format!("Agent execution failed: {}", e))?
                } else {
                    // Legacy: no bus, sub-agent output only in response
                    executor
                        .execute(
                            agent,
                            &effective_model,
                            &prompt,
                            None,
                            &tool_registry,
                            &mcp_manager,
                        )
                        .await
                        .map_err(|e| format!("Agent execution failed: {}", e))?
                };

                Ok::<_, String>((result.output, result.run_id))
            })
        })
        .await
        .map_err(|e| ToolError::execution_failed(format!("Task join error: {}", e)))?
        .map_err(|e| ToolError::execution_failed(e))?;

        Ok(ToolReturn::json(serde_json::json!({
            "agent": args.agent_name,
            "response": result.0,
            "session_id": session_id.or(Some(result.1)),
            "success": true
        })))
    }
}

/// Executor for list_agents that returns available agents.
struct ListAgentsExecutor {
    db_path: std::path::PathBuf,
}

impl ListAgentsExecutor {
    fn new(db: &Database) -> Self {
        Self {
            db_path: db.path().to_path_buf(),
        }
    }

    fn definition() -> ToolDefinition {
        ToolDefinition::new(
            "list_agents",
            "List all available agents that can be invoked.",
        )
    }
}

#[async_trait]
impl serdes_ai_agent::ToolExecutor<()> for ListAgentsExecutor {
    async fn execute(
        &self,
        _args: JsonValue,
        _ctx: &serdes_ai_agent::RunContext<()>,
    ) -> Result<ToolReturn, ToolError> {
        // Open database and list agents (we don't actually need db for listing)
        let agent_manager = AgentManager::new();
        let agents: Vec<_> = agent_manager
            .list()
            .iter()
            .map(|info| serde_json::json!({
                "name": info.name,
                "display_name": info.display_name,
                "description": info.description
            }))
            .collect();

        Ok(ToolReturn::json(serde_json::json!({
            "agents": agents,
            "count": agents.len()
        })))
    }
}

/// Executor for running SpotAgents with serdesAI's agent loop.
///
/// This replaces raw model calls with proper agent execution including:
/// - Tool calling and execution loop
/// - Streaming support
/// - Message history management
/// - Retry logic
pub struct AgentExecutor<'a> {
    db: &'a Database,
    registry: &'a ModelRegistry,
    /// Optional message bus for event publishing.
    bus: Option<MessageSender>,
}

impl<'a> AgentExecutor<'a> {
    /// Create a new executor with database access (for OAuth tokens) and model registry.
    pub fn new(db: &'a Database, registry: &'a ModelRegistry) -> Self {
        Self { db, registry, bus: None }
    }

    /// Add message bus for event publishing.
    ///
    /// When a bus is configured, sub-agent invocations will publish their
    /// events to the same bus, making nested agent output visible.
    pub fn with_bus(mut self, sender: MessageSender) -> Self {
        self.bus = Some(sender);
        self
    }

    /// Filter tool names based on settings.
    /// 
    /// Filters out:
    /// - `share_your_reasoning` unless `show_reasoning` is enabled
    /// - `invoke_agent` and `list_agents` (these use custom executors)
    fn filter_tools<'b>(&self, tool_names: Vec<&'b str>) -> Vec<&'b str> {
        let settings = Settings::new(self.db);
        let show_reasoning = settings.get_bool("show_reasoning").unwrap_or(false);
        
        tool_names
            .into_iter()
            .filter(|name| {
                match *name {
                    "share_your_reasoning" => show_reasoning,
                    // These are handled by custom executors, not the registry
                    "invoke_agent" | "list_agents" => false,
                    _ => true,
                }
            })
            .collect()
    }

    /// Check if agent wants invoke_agent tool.
    fn wants_invoke_agent(&self, tool_names: &[&str]) -> bool {
        tool_names.contains(&"invoke_agent")
    }

    /// Check if agent wants list_agents tool.
    fn wants_list_agents(&self, tool_names: &[&str]) -> bool {
        tool_names.contains(&"list_agents")
    }

    /// Execute an agent with a prompt (blocking mode).
    ///
    /// This runs the full agent loop including tool calls until completion.
    /// Returns the final output and message history for context.
    pub async fn execute(
        &self,
        spot_agent: &dyn SpotAgent,
        model_name: &str,
        prompt: &str,
        message_history: Option<Vec<ModelRequest>>,
        tool_registry: &SpotToolRegistry,
        _mcp_manager: &McpManager,
    ) -> Result<ExecutorResult, ExecutorError> {
        // Get the model (handles OAuth models and custom endpoints)
        let model = get_model(self.db, model_name, self.registry).await?;
        let wrapped_model = ArcModel(model);
        
        // Get original tool list (before filtering) to check for special tools
        let original_tools = spot_agent.available_tools();
        let wants_invoke = self.wants_invoke_agent(&original_tools);
        let wants_list = self.wants_list_agents(&original_tools);
        
        // Get the tools this agent should have access to (filtered by settings)
        let tool_names = self.filter_tools(original_tools);
        let tools = tool_registry.tools_by_name(&tool_names);
        
        // Build the serdesAI agent
        let mut builder = agent(wrapped_model)
            .system_prompt(spot_agent.system_prompt())
            .temperature(0.7)
            .max_tokens(4096);
        
        // Register built-in tools with real executors
        for tool in tools {
            let def = tool.definition();
            builder = builder.tool_with_executor(
                def,
                ToolExecutorAdapter::new(Arc::clone(&tool)),
            );
        }
        
        // Add invoke_agent with custom executor (has database access)
        if wants_invoke {
            let invoke_executor = if let Some(ref bus) = self.bus {
                InvokeAgentExecutor::new(self.db, model_name, bus.clone())
            } else {
                InvokeAgentExecutor::new_legacy(self.db, model_name)
            };
            builder = builder.tool_with_executor(
                InvokeAgentExecutor::definition(),
                invoke_executor,
            );
        }
        
        // Add list_agents with custom executor
        if wants_list {
            builder = builder.tool_with_executor(
                ListAgentsExecutor::definition(),
                ListAgentsExecutor::new(self.db),
            );
        }
        
        // Add MCP tools
        let mcp_tools = self.collect_mcp_tools(_mcp_manager).await;
        for (def, tool) in mcp_tools {
            builder = builder.tool_with_executor(
                def,
                ToolExecutorAdapter::new(tool),
            );
        }
        
        let serdes_agent = builder.build();
        
        // Set up run options with message history if provided
        let options = match message_history {
            Some(history) => RunOptions::new().message_history(history),
            None => RunOptions::new(),
        };
        
        // Run the agent
        let result = serdes_agent.run_with_options(prompt, (), options).await
            .map_err(|e| ExecutorError::Execution(e.to_string()))?;
        
        Ok(ExecutorResult {
            output: result.output.clone(),
            messages: result.messages,
            run_id: result.run_id,
        })
    }

    /// Execute agent with events published to message bus.
    ///
    /// This is the preferred method when you have a UI that subscribes
    /// to the message bus. All streaming events are converted to Messages
    /// and published, allowing sub-agents to also be visible.
    ///
    /// # Errors
    ///
    /// Returns an error if no message bus is configured (use `with_bus()` first).
    pub async fn execute_with_bus(
        &self,
        spot_agent: &dyn SpotAgent,
        model_name: &str,
        prompt: &str,
        message_history: Option<Vec<ModelRequest>>,
        tool_registry: &SpotToolRegistry,
        mcp_manager: &McpManager,
    ) -> Result<ExecutorResult, ExecutorError> {
        let bus = self.bus.as_ref().ok_or(ExecutorError::Config(
            "No message bus configured. Use with_bus() first.".into(),
        ))?;

        // Create event bridge for this agent
        let mut bridge = EventBridge::new(
            bus.clone(),
            spot_agent.name(),
            spot_agent.display_name(),
        );

        bridge.agent_started();

        // Use internal streaming execution
        let mut stream = self
            .execute_stream(
                spot_agent,
                model_name,
                prompt,
                message_history,
                tool_registry,
                mcp_manager,
            )
            .await?;

        // Accumulate text for the final output (since RunComplete only has run_id)
        let mut accumulated_text = String::new();
        let mut final_run_id: Option<String> = None;

        // Process all events through the bridge
        while let Some(event_result) = stream.recv().await {
            match event_result {
                Ok(event) => {
                    // Capture text deltas for final output
                    if let StreamEvent::TextDelta { ref text } = event {
                        accumulated_text.push_str(text);
                    }

                    // Capture run_id from RunComplete
                    if let StreamEvent::RunComplete { ref run_id } = event {
                        final_run_id = Some(run_id.clone());
                    }

                    bridge.process(event);
                }
                Err(e) => {
                    bridge.agent_error(&e.to_string());
                    return Err(e);
                }
            }
        }

        // Get the run_id (from RunComplete event)
        let run_id = final_run_id.ok_or_else(|| {
            ExecutorError::Execution("Stream ended without RunComplete event".into())
        })?;

        bridge.agent_completed(&run_id);

        // Note: In streaming mode, we don't have access to the full message history.
        // For sub-agent invocation, this is acceptable - the main agent tracks its own history.
        Ok(ExecutorResult {
            output: accumulated_text,
            messages: Vec::new(), // Not available in streaming mode
            run_id,
        })
    }

    /// Execute an agent with streaming output.
    ///
    /// **Note**: For new code, prefer [`execute_with_bus`] which automatically
    /// publishes events to a message bus that renderers can subscribe to.
    /// This method is useful when you need direct control over event handling.
    ///
    /// Returns a stream receiver for consuming events in real-time.
    /// Uses a channel internally to handle the lifetime issues with agent streams.
    ///
    /// # Example
    /// ```ignore
    /// let registry = SpotToolRegistry::new();
    /// let mut stream = executor.execute_stream(agent, model, prompt, None, &registry).await?;
    /// while let Some(event) = stream.recv().await {
    ///     match event? {
    ///         StreamEvent::TextDelta { text } => print!("{}", text),
    ///         StreamEvent::ToolCallStart { tool_name, .. } => println!("ðŸ”§ Calling: {}", tool_name),
    ///         _ => {}
    ///     }
    /// }
    /// ```
    pub async fn execute_stream(
        &self,
        spot_agent: &dyn SpotAgent,
        model_name: &str,
        prompt: &str,
        message_history: Option<Vec<ModelRequest>>,
        tool_registry: &SpotToolRegistry,
        mcp_manager: &McpManager,
    ) -> Result<ExecutorStreamReceiver, ExecutorError> {
        // Get the model (handles OAuth models and custom endpoints)
        let model = get_model(self.db, model_name, self.registry).await?;

        // Get original tool list (before filtering) to check for special tools
        let original_tools = spot_agent.available_tools();
        let wants_invoke = self.wants_invoke_agent(&original_tools);
        let wants_list = self.wants_list_agents(&original_tools);
        
        // Get the tools this agent should have access to (filtered by settings)
        let tool_names = self.filter_tools(original_tools);
        let tools = tool_registry.tools_by_name(&tool_names);
        
        // Collect tool definitions and Arc references
        // We need to move these into the spawned task
        let mut tool_data: Vec<(ToolDefinition, Arc<dyn Tool + Send + Sync>)> = tools
            .into_iter()
            .map(|t| (t.definition(), t))
            .collect();
        
        // Collect MCP tools from running servers
        let mcp_tool_calls = self.collect_mcp_tools(mcp_manager).await;
        tool_data.extend(mcp_tool_calls);
        
        // Prepare data for the spawned task
        let system_prompt = spot_agent.system_prompt();
        let prompt = prompt.to_string();
        let model_name_owned = model_name.to_string();
        let db_path = self.db.path().to_path_buf();
        let bus = self.bus.clone(); // Clone bus for sub-agent visibility
        let (tx, rx) = mpsc::channel(32);
        
        debug!(tool_count = tool_data.len(), "Spawning streaming task");
        
        // Spawn a task that owns the agent and sends events through the channel
        tokio::spawn(async move {
            debug!("Streaming task started");
            
            let wrapped_model = ArcModel(model);
            
            // Build the serdesAI agent
            debug!("Building serdesAI agent");
            let mut builder = agent(wrapped_model)
                .system_prompt(system_prompt)
                .temperature(0.7)
                .max_tokens(4096);
            
            // Register tools with real executors
            for (def, tool) in tool_data {
                debug!(tool_name = %def.name, "Registering tool");
                builder = builder.tool_with_executor(
                    def,
                    ToolExecutorAdapter::new(tool),
                );
            }
            
            // Add invoke_agent with custom executor (has database access)
            if wants_invoke {
                let invoke_executor = InvokeAgentExecutor {
                    db_path: db_path.clone(),
                    current_model: model_name_owned.clone(),
                    bus: bus.clone(), // Pass bus for sub-agent visibility
                };
                builder = builder.tool_with_executor(
                    InvokeAgentExecutor::definition(),
                    invoke_executor,
                );
            }
            
            // Add list_agents with custom executor
            if wants_list {
                builder = builder.tool_with_executor(
                    ListAgentsExecutor::definition(),
                    ListAgentsExecutor { db_path: db_path.clone() },
                );
            }
            
            let serdes_agent = builder.build();
            debug!("Agent built successfully");
            
            // Set up run options
            let history_len = message_history.as_ref().map(|h| h.len()).unwrap_or(0);
            debug!(history_messages = history_len, "Setting up run options");
            
            let options = match message_history {
                Some(history) => RunOptions::new().message_history(history),
                None => RunOptions::new(),
            };
            
            // Use real streaming from serdesAI
            debug!(prompt_len = prompt.len(), "Calling run_stream_with_options");
            
            match serdes_agent.run_stream_with_options(prompt, (), options).await {
                Ok(mut stream) => {
                    debug!("Stream started, forwarding events");
                    let mut event_count = 0u32;
                    
                    // Forward all events from the stream
                    while let Some(event_result) = stream.next().await {
                        event_count += 1;
                        match event_result {
                            Ok(event) => {
                                debug!(event_num = event_count, "Received stream event");
                                if tx.send(Ok(event)).await.is_err() {
                                    warn!("Receiver dropped, stopping stream");
                                    break;
                                }
                            }
                            Err(e) => {
                                let error_str = e.to_string();
                                error!(error = %error_str, "Stream error");
                                
                                // Log common error patterns
                                if error_str.contains("status: 400") {
                                    error!("HTTP 400 Bad Request - likely invalid model name");
                                } else if error_str.contains("status: 401") {
                                    error!("HTTP 401 Unauthorized - token may be expired");
                                } else if error_str.contains("status: 404") {
                                    error!("HTTP 404 Not Found - model name may be invalid");
                                }
                                
                                let _ = tx.send(Ok(StreamEvent::Error { 
                                    message: error_str.clone() 
                                })).await;
                                let _ = tx.send(Err(ExecutorError::Execution(error_str))).await;
                                break;
                            }
                        }
                    }
                    debug!(total_events = event_count, "Stream completed");
                }
                Err(e) => {
                    let error_str = e.to_string();
                    error!(error = %error_str, "Failed to start stream");
                    
                    // Check for common error patterns
                    if error_str.contains("status: 400") {
                        error!("HTTP 400 Bad Request - likely invalid model name or request format");
                    } else if error_str.contains("status: 401") {
                        error!("HTTP 401 Unauthorized - token may be expired or invalid");
                    } else if error_str.contains("status: 403") {
                        error!("HTTP 403 Forbidden - token may not have required permissions");
                    } else if error_str.contains("status: 404") {
                        error!("HTTP 404 Not Found - model name may be invalid");
                    } else if error_str.contains("status: 429") {
                        error!("HTTP 429 Rate Limited - too many requests");
                    } else if error_str.contains("status: 5") {
                        error!("HTTP 5xx Server Error - API issue");
                    }
                    
                    // Log the full error body if present
                    if error_str.contains("body:") {
                        error!("API Error Body: {}", error_str);
                    }
                    
                    // Send error event
                    let _ = tx.send(Ok(StreamEvent::Error { 
                        message: error_str.clone() 
                    })).await;
                    let _ = tx.send(Err(ExecutorError::Execution(error_str))).await;
                }
            }
            debug!("Streaming task exiting");
        });
        
        Ok(ExecutorStreamReceiver { rx })
    }
}



/// Result from agent execution.
#[derive(Debug, Clone)]
pub struct ExecutorResult {
    /// The agent's final text output.
    pub output: String,
    /// Full message history (for context continuation).
    pub messages: Vec<ModelRequest>,
    /// Unique run ID for tracing.
    pub run_id: String,
}

/// Receiver for streaming events from agent execution.
/// 
/// This wraps an mpsc receiver and provides a convenient interface
/// for consuming streaming events.
pub struct ExecutorStreamReceiver {
    rx: mpsc::Receiver<Result<StreamEvent, ExecutorError>>,
}

impl ExecutorStreamReceiver {
    /// Receive the next event from the stream.
    /// 
    /// Returns `None` when the stream is complete.
    pub async fn recv(&mut self) -> Option<Result<StreamEvent, ExecutorError>> {
        self.rx.recv().await
    }
}

/// Get a model by name, handling custom endpoints, OAuth models, and standard models.
///
/// This function checks the model registry first for custom configurations,
/// then falls back to OAuth detection by prefix, and finally standard inference.
pub async fn get_model(
    db: &Database,
    model_name: &str,
    registry: &ModelRegistry,
) -> Result<Arc<dyn Model>, ExecutorError> {
    debug!(model_name = %model_name, "get_model called");

    // First, check if we have a custom config for this model in the registry
    if let Some(config) = registry.get(model_name) {
        debug!(
            model_name = %model_name,
            model_type = %config.model_type,
            has_custom_endpoint = config.custom_endpoint.is_some(),
            "Found model in registry"
        );
        
        // Handle custom endpoint models (e.g., from models.dev)
        if let Some(endpoint) = &config.custom_endpoint {
            debug!(
                endpoint_url = %endpoint.url,
                has_api_key = endpoint.api_key.is_some(),
                "Custom endpoint details"
            );
            debug!("Using custom endpoint for model: {}", model_name);

            // Resolve the API key from database or environment
            let api_key = if let Some(ref key_template) = endpoint.api_key {
                if key_template.starts_with('$') {
                    // It's an env var reference like $API_KEY or ${API_KEY}
                    let var_name = key_template
                        .trim_start_matches('$')
                        .trim_matches(|c| c == '{' || c == '}');
                    // Check database first, then environment
                    resolve_api_key(db, var_name).ok_or_else(|| {
                        ExecutorError::Config(format!(
                            "API key {} not found. Run /add_model to configure it, or set the environment variable.",
                            var_name
                        ))
                    })?
                } else {
                    // It's a literal key
                    key_template.clone()
                }
            } else {
                return Err(ExecutorError::Config(format!(
                    "Model {} has custom endpoint but no API key configured",
                    model_name
                )));
            };

            // Get the actual model ID to send to the API
            let model_id = config.model_id.as_deref().unwrap_or(model_name);

            // Create OpenAI-compatible model with custom endpoint
            let model = OpenAIChatModel::new(model_id, api_key).with_base_url(&endpoint.url);

            info!(
                model_name = %model_name,
                endpoint = %endpoint.url,
                "Custom endpoint model ready"
            );
            return Ok(Arc::new(model));
        }

        // Handle based on model type for non-custom-endpoint models
        match config.model_type {
            ModelType::ClaudeCode => {
                debug!("Detected Claude Code OAuth model from config");
                let model = auth::get_claude_code_model(db, model_name)
                    .await
                    .map_err(|e| ExecutorError::Auth(e.to_string()))?;
                return Ok(Arc::new(model));
            }
            ModelType::ChatgptOauth => {
                debug!("Detected ChatGPT OAuth model from config");
                let model = auth::get_chatgpt_model(db, model_name)
                    .await
                    .map_err(|e| ExecutorError::Auth(e.to_string()))?;
                return Ok(Arc::new(model));
            }
            // For other types, fall through to standard handling
            _ => {}
        }
    }

    // Legacy: Check for OAuth models by prefix (backward compatibility)
    if model_name.starts_with("chatgpt-") || model_name.starts_with("chatgpt_") {
        debug!("Detected ChatGPT OAuth model by prefix");
        let model = auth::get_chatgpt_model(db, model_name)
            .await
            .map_err(|e| {
                error!(error = %e, "Failed to get ChatGPT model");
                ExecutorError::Auth(e.to_string())
            })?;
        info!(model_id = %model.identifier(), "ChatGPT OAuth model ready");
        return Ok(Arc::new(model));
    }

    if model_name.starts_with("claude-code-") || model_name.starts_with("claude_code_") {
        debug!("Detected Claude Code OAuth model by prefix");
        let model = auth::get_claude_code_model(db, model_name)
            .await
            .map_err(|e| {
                error!(error = %e, "Failed to get Claude Code model");
                ExecutorError::Auth(e.to_string())
            })?;
        info!(model_id = %model.identifier(), "Claude Code OAuth model ready");
        return Ok(Arc::new(model));
    }

    // Check if this looks like a custom model (provider:model format)
    // If so, it should have been in the registry - error out
    if model_name.contains(':') && !model_name.starts_with("claude-code") {
        warn!(
            model_name = %model_name,
            registry_count = registry.len(),
            "Custom model not found in registry"
        );
        return Err(ExecutorError::Config(format!(
            "Model '{}' not found in registry. Did you add it with /add-model? Try running /add-model again.",
            model_name
        )));
    }

    // Standard model inference (uses API keys from environment)
    debug!("Using API key model inference for: {}", model_name);
    let model = infer_model(model_name).map_err(|e| {
        error!(error = %e, "Failed to infer model");
        ExecutorError::Model(e.to_string())
    })?;

    info!(model_name = %model_name, "Model ready");
    Ok(model)
}

/// Legacy execute_agent function for backwards compatibility.
///
/// Prefer using `AgentExecutor` directly for new code.
#[deprecated(since = "0.2.0", note = "Use AgentExecutor::execute() instead")]
pub async fn execute_agent(
    db: &Database,
    agent: &dyn SpotAgent,
    model_name: &str,
    prompt: &str,
    message_history: &mut Vec<ModelRequest>,
) -> Result<String, ExecutorError> {
    let model_registry = ModelRegistry::load_from_db(db).unwrap_or_default();
    let executor = AgentExecutor::new(db, &model_registry);
    let tool_registry = SpotToolRegistry::new();
    let mcp_manager = McpManager::new();

    // Convert mutable history to owned
    let history = if message_history.is_empty() {
        None
    } else {
        Some(message_history.clone())
    };

    let result = executor
        .execute(
            agent,
            model_name,
            prompt,
            history,
            &tool_registry,
            &mcp_manager,
        )
        .await?;

    // Update the caller's history
    *message_history = result.messages;

    Ok(result.output)
}

impl<'a> AgentExecutor<'a> {
    /// Collect MCP tools from running servers.
    async fn collect_mcp_tools(
        &self,
        mcp_manager: &McpManager,
    ) -> Vec<(ToolDefinition, Arc<dyn Tool + Send + Sync>)> {
        let mut tools = Vec::new();
        
        // Get all tools from running MCP servers
        let all_mcp_tools = mcp_manager.list_all_tools().await;
        
        for (server_name, server_tools) in all_mcp_tools {
            for mcp_tool in server_tools {
                // Create a tool definition from MCP tool
                let def = ToolDefinition::new(
                    mcp_tool.name.clone(),
                    mcp_tool.description.clone().unwrap_or_default(),
                ).with_parameters(mcp_tool.input_schema.clone());
                
                // Create an MCP tool executor
                let executor = McpToolExecutor {
                    server_name: server_name.clone(),
                    tool_name: mcp_tool.name.clone(),
                    mcp_manager_ptr: mcp_manager as *const McpManager,
                };
                
                tools.push((def, Arc::new(executor) as Arc<dyn Tool + Send + Sync>));
            }
        }
        
        tools
    }
}

/// Tool executor that calls MCP server tools.
/// 
/// Note: We use a raw pointer to McpManager because we can't easily
/// share Arc across async boundaries here. The pointer is valid for
/// the duration of the executor run.
struct McpToolExecutor {
    server_name: String,
    tool_name: String,
    mcp_manager_ptr: *const McpManager,
}

// Safety: The pointer is only used during a single executor run
// where the McpManager is guaranteed to outlive the tool executor.
unsafe impl Send for McpToolExecutor {}
unsafe impl Sync for McpToolExecutor {}

#[async_trait]
impl Tool for McpToolExecutor {
    fn definition(&self) -> ToolDefinition {
        ToolDefinition::new(
            self.tool_name.clone(),
            format!("MCP tool from {}", self.server_name),
        )
    }

    async fn call(
        &self,
        _ctx: &RunContext<()>,
        args: JsonValue,
    ) -> Result<ToolReturn, ToolError> {
        // Safety: The McpManager outlives this executor
        let manager = unsafe { &*self.mcp_manager_ptr };
        
        match manager.call_tool(&self.server_name, &self.tool_name, args).await {
            Ok(result) => {
                // Convert MCP result to ToolReturn
                if result.is_error {
                    let error_msg = result.content
                        .first()
                        .map(|c| match c {
                            serdes_ai_mcp::ToolResultContent::Text { text } => text.clone(),
                            _ => "MCP tool error".to_string(),
                        })
                        .unwrap_or_else(|| "Unknown error".to_string());
                    Ok(ToolReturn::error(error_msg))
                } else {
                    let text = result.content
                        .into_iter()
                        .filter_map(|c| match c {
                            serdes_ai_mcp::ToolResultContent::Text { text } => Some(text),
                            _ => None,
                        })
                        .collect::<Vec<_>>()
                        .join("\n");
                    Ok(ToolReturn::text(text))
                }
            }
            Err(e) => {
                Err(ToolError::ExecutionFailed {
                    message: e.to_string(),
                    retryable: false,
                })
            }
        }
    }
}

#[derive(Debug, thiserror::Error)]
pub enum ExecutorError {
    #[error("Model error: {0}")]
    Model(String),
    #[error("Authentication error: {0}")]
    Auth(String),
    #[error("Tool error: {0}")]
    Tool(String),
    #[error("Execution error: {0}")]
    Execution(String),
    #[error("Configuration error: {0}")]
    Config(String),
}